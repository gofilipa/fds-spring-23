{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# homework 3: text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download this page by pressing the download button on upper right corner. You should save the page as an `.ipynb` file. \n",
    "\n",
    "Afer downloading the document, open it on your own computer, either on Google Colab or Jupyter-Notebooks. If using Google Colab, you may need to first upload the document to your Google Drive before you can work on it.\n",
    "\n",
    "When you are finished, download the document as an `.ipynb` file and submit it through Blackboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, you will choose a text that you've read/seen before, maybe from the Project Gutenburg library. Then, in the cells below, you will load up your text into a Jupyter notebook or Google Colab notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import the libraries we need to analyze text (nltk) and get \n",
    "# data over URLs (urllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, grab the URL of your text from Gutenberg (or elsewhere) and \n",
    "# save it to a variable called \"my_url\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the URL address with urlopen(my_url) and save it to \"opened_url\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data in the URL using read() and save it to \"raw\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our data is currently in byte type, so we need to convert it to a \n",
    "# string type using decode(). Save the result to \"decoded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will tokenize the text, then remove the front & back matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NLTK \"word_tokenize\" method to create a list of \"tokens,\"\n",
    "# or smaller strings, from our long string above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first ten words from our list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find where the first word of the text begins, this is the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the index, create a slice that excludes the frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the loop below to remove punctuation and capital leters. Make sure you save the cleaned text to a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the missing code at the comment, then run the cell. \n",
    "\n",
    "no_punct = []\n",
    "# write the first line of the for loop here\n",
    "  if word.isalpha():\n",
    "    no_punct.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the missing code at the comment, then run the cell.\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "o_unstopped = []\n",
    "for t in o_text:\n",
    "    if # complete the if statment here\n",
    "        o_unstopped.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: \n",
    "Combine all of the cleaning steps (removing stopwords, punctuation, capital letters) within a single function called \"clean\" which you can then call on the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
