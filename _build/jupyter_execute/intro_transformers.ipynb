{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM3DlhWOxRtN"
   },
   "source": [
    "# transformers: introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUPUXIFWvGQu",
    "outputId": "813bdb7c-c9e4-4870-f2fc-1a888657c55b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/caladof/anaconda3/lib/python3.11/site-packages (4.29.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /Users/caladof/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/caladof/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/caladof/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/caladof/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/caladof/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/caladof/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\r\n",
      "Requirement already satisfied: requests in /Users/caladof/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/caladof/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/caladof/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\r\n",
      "Requirement already satisfied: fsspec in /Users/caladof/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/caladof/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/caladof/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<4,>=2.5 in /Users/caladof/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/caladof/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/caladof/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\r\n"
     ]
    }
   ],
   "source": [
    "# first, install the library Transformers\n",
    "# you only need to install this library once. \n",
    "\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "c3MiVBYyxYNq"
   },
   "outputs": [],
   "source": [
    "# import the transformers library, along with the pipeline and set_seed functions\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpGPdYD6xPsE"
   },
   "source": [
    "## text generation\n",
    "generates new text based on an input prompt, like a chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434,
     "referenced_widgets": [
      "e85c539cbf8641ccb1a44367bfd9dc99",
      "52a6ab8af74342f4ba5648c598c4954d",
      "8872363a5cdb45fe82d1b43465c168f3",
      "410068b364144f0bb48014bcd273bbea",
      "2a7684c194b04b95b8a272a7dcd67dc5",
      "046caf3d165142ba848d95b49ebadbc3",
      "d9c36ed8b042466da8e47fcdbc2da425",
      "0a17f4ed5069467d839397342c72af5f",
      "a74c78d08ba14387b6b7d6084514122d",
      "b53ee14200e3489bacf9df525b28dbfa",
      "57756ecc597d43e5a413bbb52e13a168",
      "3321802876894b7594cbd687f5aafe76",
      "c83fe15b40ef40029a2d74123f546648",
      "4d1f131e02f04be1bbdca5887a87528b",
      "8f557705452244699d4cb692ed8b8e16",
      "102d0e997e214211bf79af63978e4b17",
      "d47b54f38f864280ab1bb8ab52ba6706",
      "f4f111aa445b4e0698165a5924a66068",
      "5b6227804cdd4fb5b531765ce74ebf40",
      "a935245941b349b3aae3d630e020352d",
      "7621fe2462814668ab5f0722f9bf18fa",
      "c78f82c2658f48808b78b14aee381849",
      "3564ca44d3544ca089945d37b1b8f665",
      "9d4aecc830674bda909e5ca309d45ab1",
      "b63552d6f90b49e995412f64b7335039",
      "0a3a446472ac415db135c5cfb24227ac",
      "50f6218ea1b54e7aa2fcf46d51c5f59a",
      "6bb55b21256a450aa354b35cba30ba62",
      "b01ab7cbf4084c6991655fe66b75f549",
      "feb4b48c39384330bbff6977f0b25cb3",
      "ff2361ef3c0240d69ed6d58c469a9bbe",
      "011e33bf8e1243c588116e739e8e3d55",
      "5395885936234f3e8c85306f8b121a23",
      "710aaa70ad4f4f07bfb7470d2ddd8212",
      "b481619c612842b1872c16703c3694bc",
      "4c1d281a5b11417c8dfd688970289c4a",
      "d49155fe22f24d77bdb7d3f70ea5b1cc",
      "44763535f47c40d8849e11de49e9481f",
      "dd96f6860f8640e4859f9d52de7332ad",
      "6b32d4719a254bea8ea21e0d72463d02",
      "808be5eef1a14f4aa6cc7632e27e41f7",
      "bb70bbc0b1c74b1b8ab1f7822f3c3c74",
      "6ecc0e829ff440bb86bc2a6d6e350087",
      "e5b1cc8925964e57b7843a6fd581ed6f",
      "2eeea0eb6e274ccd89f2b3753947c9aa",
      "0dc9501df18e4bb0beae84119b7dd147",
      "4db3e39d59454e2fa18c3b417b416300",
      "b8fd6ad78d304bdda35325def6680ac7",
      "2e396cae908d4154a3656c379e5aa580",
      "b9f68aab89064916aa44c048e535bd46",
      "1959024c7d3f484e965ed24c6ac26719",
      "447c0b2dacb445619547b32c6a890a34",
      "4bfe95ab42a14ca7bd1e14db13e4da78",
      "80b10292304f4eb285992c4e9f8dcbcd",
      "e262c2029f7646be974b80645f97b730",
      "8815cbcdbd2c473c9a48edeb78cf4ff9",
      "bd976b0361dd43ed901593a68e47318c",
      "8d08c7608ae448579b204633e33d64d1",
      "ab46e0da3d284134944a4da8186c7543",
      "c537dcb6e5af49018ade4111a0cfc02c",
      "e078be3db0624b82b560d51b122668d5",
      "26c438850b7b499c80eb2c1cc21396c7",
      "e1dbf79ec9bf414795612a8f15c67f79",
      "67a521dce1974b71abebc2be56131c36",
      "1bc6c64e6f37465bb7c49f96106e1702",
      "5ef799cdbcad45c3abb8fc9498d3200c"
     ]
    },
    "id": "HCGKMhpIwgI6",
    "outputId": "d8d9996f-eb83-415d-bb16-b4dd6ec3a9af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "# pulling in the text generation \"pipeline\", and setting it to the variable\n",
    "# called \"generator\"\n",
    "\n",
    "generator = pipeline('text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2Fssv7MyMCQ",
    "outputId": "3a7d9329-8990-4d1b-9de4-2e08068b7d60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'This summer, I was rock climbing in Yosemite when I heard the first of six live moose, one of many of the first live booms with my mom. For weeks as we worked to clean up after the booms, we walked around the'},\n",
       " {'generated_text': \"This summer, I was rock climbing in Yosemite when a group of volunteers showed up to see me. I wasn't on the route, but I had been working hard and had a good time.\\n\\nI got a photo of my buddy Jesse,\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking the generator function and passing a sentence and maximum length and \n",
    "# number of responses to the function\n",
    "\n",
    "generator('This summer, I was rock climbing in Yosemite when',\n",
    "          max_length=50,\n",
    "          num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpebcd7flh9g"
   },
   "source": [
    "## fill mask\n",
    "Fills the word in the blank with a guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374,
     "referenced_widgets": [
      "efaaf43944fe4988a3267ee19c7da22e",
      "3c0036d8c6b5441481a5819bf9cfd381",
      "ab6ed4c61f0c4659afdb9e3c5a37d5d2",
      "ba07c13d545a4c93bac2774af0894ce7",
      "535145f3bcbb44da95f985762c556439",
      "58d5a17705d243289d64b3346a24a021",
      "68e1e13e2c414cbe92ae8b953fdc5f86",
      "c998b1f4793f41be88ce836c3cce03b9",
      "bf64343185a342dab5e713c74711d3ab",
      "fa3dfe83f06d4d7b9fc4389d78da33e6",
      "d45a8386d36b48b588e3899dbaf3d881",
      "e9557587ee3c449ca771d4a3941e3f41",
      "e52cacc1a38f4de1931212cb5d53dea0",
      "a0b1afb4bb1c426ea2a1ce41e8ec3784",
      "020eabf924b94a69a45173f26fa04b30",
      "ac9ce2ae9578455786c2483c4a3d7eb5",
      "b768a517ff044d69b8ea4f57c7f3cc17",
      "2e93ab55b56a493a830d0aa6b27c8e63",
      "b3a9740116fb4e39bb3460d899a82761",
      "46faa76bb3694472a4e8837616392b1f",
      "cdadfee5bdf94b48b0e8470aed76bf60",
      "2504f5851cb748f69263eed129bc76b1",
      "0dad6820d47744e6a4bf5a39fc6c433a",
      "4d40a7741c4a4e32b32097d51b65fc03",
      "0b9ff79e88f84d1fa915d5c82ae84598",
      "e5de1fea2b1a456ca058ed87df53365d",
      "6306e4116d784b7dad6bdbeb1418c7b5",
      "324997d14a954b109a2acaf9f8c807ab",
      "b171e13e5abb4430bbbbb00d61868c5e",
      "fc9348493572474b95caf05b2ea7a96d",
      "b140c8b71bd1416091034318915eb823",
      "027bf2d9a8c746ddbb9b2536e28f0784",
      "390a6d0af1ab4b28adafe0a07327105b",
      "4c581e6b4b494b9eaf0a0cf5401e8871",
      "1806c084382441a7bdbfedc8b2f530f3",
      "c8c2b46407c4489d94fb9c88a89e8a0f",
      "a006ac25c9454e79ada23161a8d3ef12",
      "667cc5b75eb143fe8cafa6bb182d2119",
      "a5ff438c4bf14ddbb96854d03b6b3fc6",
      "727fd9f90c94423bb70a9406ad013ed4",
      "bce7d8bed6e04f6c929e2d104b6e4a4c",
      "e902ac3b45574c15b76fb04d9f781bee",
      "d5f2353670404156b40e1eda9c639806",
      "2138a20bc1a84fc0929de5268ae04acc",
      "09f6f3d49e97497bb400d5811ddec7f9",
      "f0e8ce1c7f164544a2c82f41584ae6ac",
      "7f44d75d657e43f0b9ea4e5af3e70dfa",
      "bd3cacfda33c42d49e8881210d4cde67",
      "21762f60cd7d43f485ac0e703a9243f0",
      "f927d1723cc1460cb90c81975befff90",
      "17cf6e30942a4583853692ca114b694f",
      "ccb6c9d39a3a49a9b0c53d3e0792a272",
      "833a501542e74784903e186cbf63ef9c",
      "790c0346e83b4802ada2079768b2bc19",
      "fc142a69f62f4c5fb1792f7db85c45da"
     ]
    },
    "id": "Rb-CFBvBy5xX",
    "outputId": "383a7677-fb27-4988-f2e8-fd9139ceea81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e2951824db481fa418475a801a4746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4185cd12678e41c09f98ecf182a236e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc9299257f2459a8ff2ee4755ad372a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e940c74f6022404c916298079902dfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0244792afb420db00aeb3beadc1d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create the \"unmasker\" variable set to the \"fill-mask\" task\n",
    "\n",
    "unmasker = pipeline('fill-mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpVu1A74lqia",
    "outputId": "b690f35d-e924-4b2b-9d29-60a1bd79e6f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.10899507254362106,\n",
       "  'token': 2249,\n",
       "  'token_str': ' difference',\n",
       "  'sequence': 'To be or not to be; that is the difference'},\n",
       " {'score': 0.057924505323171616,\n",
       "  'token': 2031,\n",
       "  'token_str': ' choice',\n",
       "  'sequence': 'To be or not to be; that is the choice'},\n",
       " {'score': 0.05728177726268768,\n",
       "  'token': 3157,\n",
       "  'token_str': ' truth',\n",
       "  'sequence': 'To be or not to be; that is the truth'},\n",
       " {'score': 0.04440455138683319,\n",
       "  'token': 1948,\n",
       "  'token_str': ' answer',\n",
       "  'sequence': 'To be or not to be; that is the answer'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# give it a sentence, with the <mask> as a fill in the blank\n",
    "# the \"top_k\" argument means we will get 4 responses\n",
    "\n",
    "unmasker('To be or not to be; that is the <mask>', top_k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zhoMyHw-mYWV",
    "outputId": "275954a6-87cf-4902-f314-4a94c23b0aa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.13512930274009705,\n",
       "  'token': 20124,\n",
       "  'token_str': ' MIT',\n",
       "  'sequence': 'My name is Professor Calado and I teach at MIT'},\n",
       " {'score': 0.07084149122238159,\n",
       "  'token': 10441,\n",
       "  'token_str': ' UCLA',\n",
       "  'sequence': 'My name is Professor Calado and I teach at UCLA'},\n",
       " {'score': 0.06717373430728912,\n",
       "  'token': 8607,\n",
       "  'token_str': ' Stanford',\n",
       "  'sequence': 'My name is Professor Calado and I teach at Stanford'},\n",
       " {'score': 0.06465483456850052,\n",
       "  'token': 23706,\n",
       "  'token_str': ' BYU',\n",
       "  'sequence': 'My name is Professor Calado and I teach at BYU'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker('My name is Professor Calado and I teach at <mask>', top_k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKsE7gyFqB_Q"
   },
   "source": [
    "## summarization\n",
    "Takes a longer text and condenses it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466,
     "referenced_widgets": [
      "eeea02abc85247dbb0d1fd6aec9bf9c6",
      "6f250da5e26a40e299d4d0d1a9ad8c34",
      "3a4b05c62bb645ddaf597d354114a1e9",
      "edc9215d712547d293c44929369b644b",
      "878f7aa4848843b5be3fda8a4b91776f",
      "537438a0daac45ea9b5d57994068c021",
      "e187bb67df8c44f79344b105c445bfa1",
      "a003e344d64044e1993e5cec6c3e1ffc",
      "18cd9df6f32d44368f1700401ccd8089",
      "b211fbbec4e440799211a1c4c64bca23",
      "6fa02f90bc8f4c1ca36241958bd1fccd",
      "41d0f7423d6445c7b593de3703b37d3d",
      "452ba1095cfe41128f3471079beafe8f",
      "b92afca3c00544358cb873faa26f579f",
      "5162db0f0e194e5793de3f2b31ea14e6",
      "1327af40cf674a14aaf9683830e5c383",
      "f978186b3e42401d93ca65e1de97e551",
      "5977c3740c4846ae99d293c4954501cd",
      "8c98c70e51a744e09ce280de072ee73d",
      "82dedfa2d61f434da8efa1694d0f8e9a",
      "449766c26526457ba3c801ce34c4b8c3",
      "efa3689ead17401d84ff6275cf87d5ca",
      "86be71dd98264079b8dbd70d28e5db1d",
      "310ee11a5625438883f5c9dc0af0ca85",
      "f7d876d44aa841e2a2af614d5ac5dedb",
      "3f330e09502f402480bb29d990699dcd",
      "2945733e9e834d4880da8aea37881a73",
      "8335292d31cb4d7b8cf1880dfaf001e2",
      "29886b5a4adb49b39b8d3fd9c122cf74",
      "a2f482bc3da34ce0879e088066b2cd29",
      "53830e4c13d24597946dcc736d92ec4b",
      "df985f49fec543e89fb862d5b5a77f9b",
      "ffc29fc3a52b43e190b0ba863719b9f5",
      "8f0bf6f8647b43ae8e8da8165b14a403",
      "4f160b36039644209f250f338f916415",
      "9321c67e2cb143f9a657faae2f72c78e",
      "72bbb181b09646e0961543e40a7fafd3",
      "0bfb73f2d13048b0b5e018ed8b8d49c8",
      "aeba5864696c486183ee030b880c0e6f",
      "df4b882286134486ac9c254631d80d10",
      "27f5a5781ef248558258f633328d0326",
      "54f16551dcb3468dac18aebec9638efd",
      "ba5f26a2512141b68c1d4c0d987bbaf0",
      "33ea25d1b70d497499cc33ffc0738e81",
      "c8028fc6d2db49a68e4f733126afa175",
      "88935481c25a495aafefdc3e22794ea2",
      "776bc43a4ec24bd7b9018044a826f3f8",
      "231f57034a5a4465a4944950417a42c7",
      "38dd19dc92284b8d9e6cdd88a08975f2",
      "ed0fdaa451714c86ac560caa28beffb7",
      "fe3b67fe7e9e493c9ab1cd27c5f4a0d4",
      "839dbf2f0b174514b41f477d47ffc5eb",
      "280cd853723a40cd855c1a84301d7055",
      "a2323fa40dcc40b09194bf17a2bfb0eb",
      "1d680a2a66ed48f08dc4c5dc56e8a66c"
     ]
    },
    "id": "eSNrJUFBmzmR",
    "outputId": "6e1f8f52-c3e1-4899-94e1-108ae97c18f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5523bd9c9bc47f38cc9b331da93a69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3658063785d49efb4015e23ffda11f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# taking the \"summarization\" task and saving it to \"summarizer\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# then passing some text into the \"summarizer\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# we use three quotes at the beginning and end of the string \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# if we want to put in a text that spans multiple lines\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m summarizer(\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mThe past 3 years of work in NLP have been characterized \u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mby the development and deployment of ever larger language models, \u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mespecially for English. BERT, its variants, GPT-2/3, and others, \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124mresearch and development goals and supports stakeholder values, and \u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124mencouraging research directions beyond ever larger language models.\u001b[39m\u001b[38;5;124m'''\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/__init__.py:788\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;66;03m# Infer the framework from the model\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Will load the correct model if possible\u001b[39;00m\n\u001b[1;32m    787\u001b[0m model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 788\u001b[0m framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    789\u001b[0m     model,\n\u001b[1;32m    790\u001b[0m     model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[1;32m    791\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    792\u001b[0m     framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[1;32m    793\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    796\u001b[0m )\n\u001b[1;32m    798\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    799\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:270\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    272\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:467\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    466\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    468\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    469\u001b[0m     )\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:2432\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2417\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2418\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   2419\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2420\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   2421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   2431\u001b[0m     }\n\u001b[0;32m-> 2432\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs)\n\u001b[1;32m   2434\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   2435\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   2436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   2437\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    414\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[1;32m    420\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    421\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    422\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    423\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    424\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    425\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    426\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    427\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    428\u001b[0m         use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_file_manager() \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[1;32m   1362\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, temp_file\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m-> 1364\u001b[0m     http_get(\n\u001b[1;32m   1365\u001b[0m         url_to_download,\n\u001b[1;32m   1366\u001b[0m         temp_file,\n\u001b[1;32m   1367\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1368\u001b[0m         resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[1;32m   1369\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1370\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:544\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m         progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n\u001b[0;32m--> 544\u001b[0m         temp_file\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m expected_size \u001b[38;5;241m!=\u001b[39m temp_file\u001b[38;5;241m.\u001b[39mtell():\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsistency check failed: file should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but has size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    549\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_file\u001b[38;5;241m.\u001b[39mtell()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWe are sorry for the inconvenience. Please retry download and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pass `force_download=True, resume_download=False` as argument.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf the issue persists, please let us\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m know by opening an issue on https://github.com/huggingface/huggingface_hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    552\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/tempfile.py:483\u001b[0m, in \u001b[0;36m_TemporaryFileWrapper.__getattr__.<locals>.func_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;129m@_functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# taking the \"summarization\" task and saving it to \"summarizer\"\n",
    "# then passing some text into the \"summarizer\"\n",
    "\n",
    "# we use three quotes at the beginning and end of the string \n",
    "# if we want to put in a text that spans multiple lines\n",
    "\n",
    "summarizer = pipeline('summarization')\n",
    "summarizer('''The past 3 years of work in NLP have been characterized \n",
    "by the development and deployment of ever larger language models, \n",
    "especially for English. BERT, its variants, GPT-2/3, and others, \n",
    "most recently Switch-C, have pushed the boundaries of the possible \n",
    "both through architectural innovations and through sheer size. Using \n",
    "these pretrained models and the methodology of fine-tuning them for \n",
    "specific tasks, researchers have extended the state of the art on a \n",
    "wide array of tasks as measured by leaderboards on specific benchmarks \n",
    "for English. In this paper, we take a step back and ask: How big is too \n",
    "big? What are the possible risks associated with this technology and \n",
    "what paths are available for mitigating those risks? We provide \n",
    "recommendations including weighing the environmental and financial costs \n",
    "first, investing resources into curating and carefully documenting \n",
    "datasets rather than ingesting everything on the web, carrying out \n",
    "pre-development exercises evaluating how the planned approach fits into \n",
    "research and development goals and supports stakeholder values, and \n",
    "encouraging research directions beyond ever larger language models.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYwbSsN7rMUA"
   },
   "source": [
    "## question-answering\n",
    "Takes an input question and context and provides an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxpi8p31qx76",
    "outputId": "685a048d-153d-4fcd-f373-a49ee2a86a3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6520994901657104, 'start': 71, 'end': 78, 'answer': 'a woman'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling the question-answer pipeline\n",
    "# passing a question and context into the pipeline\n",
    "# the function will look into the context to get the answer\n",
    "\n",
    "question_answer = pipeline('question-answering')\n",
    "question_answer(question='Was the writer of Frankenstien a man or a woman?', \n",
    "                context='''Frankenstien is a book written by Mary Shelley who is \n",
    "                a woman''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eu4_q9uMrbVG"
   },
   "source": [
    "## ner (named entity recognition)\n",
    "Named entity recognition (NER) is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521,
     "referenced_widgets": [
      "e6d2e774a63c459f9f53b35227bac4dc",
      "02f11a5c1ed5479e9cddee7e3519a913",
      "0ddbfb9544404d448e7655ced70a1899",
      "049a3534f55d45fbb64628178527f77a",
      "a8fdfc51d2e940519b7a4ff2f07353e7",
      "b8540eee926649fa9ceff8ec76251690",
      "e11c0aa83bd74553a1ca8d4ea5440294",
      "f7eaea116a6d416f95785ae87096f64c",
      "add61703843d40a4b102e39b61b4752b",
      "6c7fff08223545f9af34893ff43f2976",
      "0b40d06c49874f9587d581ba471c1d69",
      "22a0dceb96994afabd0c44c4968ba1cd",
      "106acf21ad4a4af08abdf6a10091e8b9",
      "93a81c580db64524bc186f847d62b929",
      "bcc7429b73004255bd9e11e0c7cde04e",
      "d40f662e6ad2464384cbb3498ce0fe8a",
      "15fff51537c541a6bd7942f1660263ed",
      "8dd1a81954e94071af7433bc0a90c17a",
      "554cfc3093a648feaca99171e6712478",
      "e90172efc5cf40ce96eec642a5273a40",
      "0e7e79d983cf4a06ae928fb4f542a753",
      "219ac95052aa4923bd5ab6eefa7bc5f6",
      "a6e3c1f389824e41bb0063e3c6f0015a",
      "572d7f7239e84845bcaba1112cd70adb",
      "1aedc3265a6a47a5b9e44c2907016505",
      "1a58262c4e5e41808d7d111399075c56",
      "b2e181fb73854ea6bc234b3da7eedc85",
      "59325570be7c4d41bfc0a3bee60e835e",
      "5d4df5ed6f4240afb178a80af2e37086",
      "507a33786b8b49ad982bbe7178074495",
      "95da5a358ca64aeaa086e615c0788bb7",
      "691b2251bdc54ad5bc42a5d111db7348",
      "f988f7bb185b4bf3aae8e264c701ccf6",
      "4133165a1f9d47f0b2d146cabe9ee98e",
      "6ca84b27029443f286f5c1bfc66245aa",
      "30ec6428f31b45caba999f7e4b3a98cf",
      "3f919545ef474d1f8fb945e2a68bb435",
      "6357ddc898444f35910f12d4650d26c9",
      "ef9e3a1fa0dc4b42a5050d09a7d12722",
      "583f501abeec416984ae5d3b5919dfae",
      "66b054e01e894f858581de68570eb854",
      "945d65f3a94a48c5a7bbc108ea5bf8aa",
      "561f3732e4ce4205a1699eab5b2e8a93",
      "96bfd99ee1694e1c858e2690825284f2"
     ]
    },
    "id": "yC0oHzufrjc9",
    "outputId": "31f3a0a3-6f6c-4c48-97c9-e12a7a3bc618"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d2e774a63c459f9f53b35227bac4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a0dceb96994afabd0c44c4968ba1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e3c1f389824e41bb0063e3c6f0015a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4133165a1f9d47f0b2d146cabe9ee98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9985998,\n",
       "  'word': 'Filipa Calado',\n",
       "  'start': 11,\n",
       "  'end': 24},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9940423,\n",
       "  'word': 'City College',\n",
       "  'start': 39,\n",
       "  'end': 51},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9883624,\n",
       "  'word': 'Manhattan',\n",
       "  'start': 55,\n",
       "  'end': 64}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Filipa Calado and I work at City College in Manhattan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koGYMQa99mki"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}