{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ea8d0f",
   "metadata": {},
   "source": [
    "## transformers: bias & discrimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e23582",
   "metadata": {},
   "source": [
    "### Freewrite: \n",
    "\n",
    "### Why is bias and discrimination a problem in large language models? Where do models learn to be biased? \n",
    "- the data from the models do not represent everyone. \n",
    "- the people programming the models: computer programmers. They are not trained to be sensitive to alternative perspectives, or minority experiences or views. \n",
    "- some of the data sources include reddit and are not well moderated\n",
    "- reflects current society, so if society is biased, then the model will be biased. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950d0e8",
   "metadata": {},
   "source": [
    "### Why is it difficult to train a model that isn't biased? Refer to the \"[Stochastic Parrots](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)\" paper, section 4, to get ideas\n",
    "  - \"Size Doesnâ€™t Guarantee Diversity\" - the idea that representation is enough to represent diverse points of view overlooks the ways that the data and training practices overlooks minority perspectives. \n",
    "  - \"Hegemonic viewpoints\" - the dominant view. The model only represents the most common viewpoints. Ideas, identites, perspectives that are different or less common get weeded out. This is because it's a statistical model, looking for the biggest pattern. \n",
    "  - Privilege - those who do not have access to the internet cannot represent themselves. People who are not in 1st world countries; people who cannot afford technologies. \n",
    "  - Most of the data comes from one demographic (male, young) whose view will dominate the others. \n",
    "  - \"Mob mentality\" on social media means that unpopular views are suppressed. \n",
    "  - There's no way to \"automate\" taking out bias. The attempt to weed out \"bad words\" takes out all examples of the words, even the ones that explain why the words are bad. This means the model has no information about why certain words are offensive. For example the the list of \"[Dirty, Naughty, Obscene or Otherwise Bad Words](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb393626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}