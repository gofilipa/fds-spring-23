{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb4b1e4e",
   "metadata": {},
   "source": [
    "# Midterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325d07d",
   "metadata": {},
   "source": [
    "## Section 1: Python Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf98970",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = [\"Imma\", \"let\", \"you\", \"finish\", \"but\", \"Beyonce\", \"had\", \n",
    "         \"the\", \"best\", \"video\", \"of\", \"all\", \"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df30775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 1:\n",
    "\n",
    "# Take the above list called \"quote\", and write a loop below to print out each word from \n",
    "# the from the list, one by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "759cba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 2:\n",
    "\n",
    "# In your own words below, explain how a loop works. What is each line of code \n",
    "# doing? 1 - 2 sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29338cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 3: \n",
    "\n",
    "# From the above quote, print out the first word using list indexing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801251ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e802d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 4:\n",
    "\n",
    "# From the above quote, print out the first 4 words using list slicing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33969bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67467f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 5:\n",
    "\n",
    "# From the above quote, print out the last word using list indexing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73863ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be2506ca",
   "metadata": {},
   "source": [
    "## text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f11df884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all computers: run this cell to import our libraries we will need for text cleaning\n",
    "\n",
    "import nltk\n",
    "from urllib.request import urlopen \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ac6d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Jupyter-Notebooks ONLY: run this cell\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b751aada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/filipacalado/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/filipacalado/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/filipacalado/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nlkt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bf3cf756f8c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'omw-1.4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnlkt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nlkt' is not defined"
     ]
    }
   ],
   "source": [
    "# for Google Colab ONLY: run this cell\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nlkt.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, run this cell to load up and format the text, which is \"Frankenstein\"\n",
    "\n",
    "my_url = 'https://www.gutenberg.org/cache/epub/84/pg84.txt'\n",
    "opened_url = urlopen(my_url)\n",
    "raw = opened_url.read()\n",
    "decoded = raw.decode()\n",
    "tokens = nltk.word_tokenize(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f23ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, run this cell to check the first ten words \"tokens\"\n",
    "\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a711e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 6:\n",
    "\n",
    "# Explain what the above code is doing in your own words, 1-2 sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ba79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "frank_no_punct_lower = [] # creating an empty list, to put new words in\n",
    "\n",
    "for word in tokens: # picking out each word in list of tokens\n",
    "    if word.isalpha(): # checking if that word is in the alphabet\n",
    "        frank_no_punct_lower.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da046731",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 7:\n",
    "\n",
    "# Explain in your own words what the loop is doing in the last\n",
    "# line. 1 - 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10815f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to load our list of stopwords\n",
    "\n",
    "stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a86f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to remove stopwords\n",
    "\n",
    "frank_no_stops = []\n",
    "\n",
    "for item in frank_no_punct_lower:\n",
    "    if item not in stops:\n",
    "        frank_no_stops.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6deb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 8: \n",
    "\n",
    "# Explain in your own words why we would want to remove stopwords from our text. How\n",
    "# might they affect our analysis of word frequencies in the text? 1 - 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfccf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR JUPYTER ONLY:\n",
    "# run this cell to create the lemmatizer variable for our next loop\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ded49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COLAB ONLY:\n",
    "# run this cell to create the lemmatizer variable for our next loop\n",
    "\n",
    "\n",
    "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609fbae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frank_lemmatized = []\n",
    "\n",
    "for word in frank_no_stops: # picking out each word in our list of words without stopwords\n",
    "    lemma = wordnet_lemmatizer.lemmatize(word) \n",
    "    frank_lemmatized.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1786fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 8:\n",
    "\n",
    "# Explain in your own words what the loop is doing on each line. I did the first line for you.\n",
    "# 1 sentence per line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "157e70d8",
   "metadata": {},
   "source": [
    "## Bonus questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45727ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, run this cell to create an NLTK type object from our current text, so we can analyze it.\n",
    "\n",
    "text = nltk.Text(frank_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31aab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, run this cell to see the 20 most common words from the cleaned text\n",
    "\n",
    "text.vocab().most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dad939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS QUESTION # 1:\n",
    "# use the NLTK method .similar() on one of the words from the above list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19110dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS QUESTION # 2:\n",
    "# use the NLTK method .condordance() on the word \"monster\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156fa91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS QUESTION # 3:\n",
    "# use the NLTK method .dispersion_plot() on two or more words of your choosing from\n",
    "# the results so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS QUESTION # 4:\n",
    "# Based on the results of the above analysis, what do you find interesting about this \n",
    "# text? What do you think the most common words in the text are telling us about the \n",
    "# text? Feel free to take a guess here, and follow your curiosity. 2 - 3 sentences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7657f387",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}