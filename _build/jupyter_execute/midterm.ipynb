{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb4e3b37",
   "metadata": {},
   "source": [
    "# Midterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559390e3",
   "metadata": {},
   "source": [
    "## Section 1: Python Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cadab768",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = [\"Imma\", \"let\", \"you\", \"finish\", \"but\", \"Beyonce\", \"had\", \n",
    "         \"the\", \"best\", \"video\", \"of\", \"all\", \"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06f5e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 1:\n",
    "\n",
    "# Take the above list called \"quote\", and write a loop below to print out each word from \n",
    "# the from the list, one by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "627434c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 2:\n",
    "\n",
    "# In your own words below, explain how a loop works. What is each line of code \n",
    "# doing? 1 - 2 sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e266ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 3: \n",
    "\n",
    "# From the above quote, print out the first word using list indexing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d235a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60df9d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 4:\n",
    "\n",
    "# From the above quote, print out the first 4 words using list slicing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf5057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c8abe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 5:\n",
    "\n",
    "# From the above quote, print out the last word using list indexing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86a425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d55440e5",
   "metadata": {},
   "source": [
    "## text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "039a99e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all computers: run this cell to import our libraries we will need for text cleaning\n",
    "\n",
    "import nltk\n",
    "from urllib.request import urlopen \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73fccb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Jupyter-Notebooks ONLY: run this cell\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0359e014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/filipacalado/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/filipacalado/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/filipacalado/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/filipacalado/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for Google Colab ONLY: run this cell\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d014656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, run this cell to load up and format the text, which is \"Frankenstein\"\n",
    "\n",
    "my_url = 'https://www.gutenberg.org/cache/epub/84/pg84.txt'\n",
    "opened_url = urlopen(my_url)\n",
    "raw = opened_url.read()\n",
    "decoded = raw.decode()\n",
    "tokens = nltk.word_tokenize(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ad9e3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'eBook',\n",
       " 'of',\n",
       " 'Frankenstein',\n",
       " ',',\n",
       " 'by',\n",
       " 'Mary',\n",
       " 'Wollstonecraft']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then, run this cell to check the first ten words \"tokens\"\n",
    "\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3a76304",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 6:\n",
    "\n",
    "# Explain what the above code is doing in your own words, 1-2 sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9ceb09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frank_no_punct_lower = [] # creating an empty list, to put new words in\n",
    "\n",
    "for word in tokens: # picking out each word in list of tokens\n",
    "    if word.isalpha(): # checking if that word is in the alphabet\n",
    "        frank_no_punct_lower.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82555ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 7:\n",
    "\n",
    "# Explain in your own words what the loop is doing in the last\n",
    "# line. 1 - 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8b190ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to load our list of stopwords\n",
    "\n",
    "stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cca22365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to remove stopwords\n",
    "\n",
    "frank_no_stops = []\n",
    "\n",
    "for item in frank_no_punct_lower:\n",
    "    if item not in stops:\n",
    "        frank_no_stops.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc280211",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 8: \n",
    "\n",
    "# Explain in your own words why we would want to remove stopwords from our text. How\n",
    "# might they affect our analysis of word frequencies in the text? 1 - 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c831e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR JUPYTER ONLY:\n",
    "# run this cell to create the lemmatizer variable for our next loop\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc10039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR COLAB ONLY:\n",
    "# run this cell to create the lemmatizer variable for our next loop\n",
    "\n",
    "\n",
    "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6bcb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frank_lemmatized = []\n",
    "\n",
    "for word in frank_no_stops: # picking out each word in our list of words without stopwords\n",
    "    lemma = wordnet_lemmatizer.lemmatize(word) \n",
    "    frank_lemmatized.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b2ddced",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 8:\n",
    "\n",
    "# Explain in your own words what the loop is doing on each line. I did the first line for you.\n",
    "# 1 sentence per line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127629c",
   "metadata": {},
   "source": [
    "## Bonus questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf801ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, run this cell to create an NLTK type object from our current text, so we can analyze it.\n",
    "\n",
    "text = nltk.Text(frank_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93c36b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 204),\n",
       " ('could', 197),\n",
       " ('would', 183),\n",
       " ('yet', 152),\n",
       " ('man', 136),\n",
       " ('father', 133),\n",
       " ('upon', 128),\n",
       " ('may', 114),\n",
       " ('life', 113),\n",
       " ('every', 109),\n",
       " ('first', 108),\n",
       " ('might', 108),\n",
       " ('shall', 106),\n",
       " ('eyes', 104),\n",
       " ('said', 102),\n",
       " ('time', 97),\n",
       " ('even', 96),\n",
       " ('towards', 94),\n",
       " ('saw', 94),\n",
       " ('found', 89)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then, run this cell to see the 20 most common words from the cleaned text\n",
    "\n",
    "text.vocab().most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0291cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS QUESTION # 1:\n",
    "# use the NLTK method .similar() on one of the words from the above list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c018760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS QUESTION # 2:\n",
    "# use the NLTK method .concordance() on the word \"monster\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3306acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS QUESTION # 3:\n",
    "# use the NLTK method .dispersion_plot() on two or more words of your choosing from\n",
    "# the results so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91c6df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS QUESTION # 4:\n",
    "# Based on the results of the above analysis, what do you find interesting about this \n",
    "# text? What do you think the most common words in the text are telling us about the \n",
    "# text? Feel free to take a guess here, and follow your curiosity. 2 - 3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32fe57",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}