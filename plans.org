** TODO Class Plans
*** DONE February 1: Types and Variables
     CLOSED: [2023-02-09 Thu 14:25]
Today's goals:
- Review last class - Calculator, Data Types
- Learn about Variables and Objects 
- Start to do Homework. Drop into Google Drive (temporary solution)
- Note on online learning: I demo, then give you time for practice. 

Review from last class:
- Google colab environment - interactive, REPL
- Cells, Running cells
- Calculator
- Data types
- type() function

New Stuff:
Variables
- How we store information. Symbol. 
- Use the equals sign to assign a value.
- Can set to integers, strings, lists. 
- Try to run type() on various variables. 
- There are some rules to making variables . Going to put you in
  groups to figure out together what the rules are. 5 minutes.

Objects
- string concatenation ("" + "")
- string formatting (f"")
- every peice of data is an object. It comes with built-in
  functionality. For strings, .upper(), .lower(). For lists, .sort(), 
- comments

*** DONE February 6: Functions
     CLOSED: [2023-02-09 Thu 14:25]
Today's goals: 
- review variables & objects
- learn about functions, writing functions, calling them
- new environment, Jupyter
- talk about blackboard

Homework: 
- issues with submitting for a few people. What kind of computer? How? 
- future homeworks will be on blackboard.

Jupyter notebooks: 
- environment like google colab, but on your computer. 

Review: 
- variables: how we store information. 
- objects: what gets created when we store. Has a specific data type,
  which limits what we can do with it. Strings can be
  uppered. Integers cannot be added to strings. 

Functions
- a way of doing something. input -> output
- defining a function: add two numbers first without params then with
- calling a function: first w/o params then with
- params, arguments, input. 
- GROUPS: create separate functions called "add" and "subtract" that
  prints out the addition and subtraction of two numbers. Share.
- CHALLENGE: create a function that prints "hello, [name]" where it
  will print any name passed from the function call. Hint: you will
  need to combine what you know about f-string formatting with
  creating and calling unctions.

*** DONE February 8: Loops
     CLOSED: [2023-02-09 Thu 14:25]
Today's goals: 
- review Functions
- learn about Loops

Take attendance.

Loops
- How we will work with data. Like functions, core programming
concepts.
    - Python works with data by /iterating/. The loop.
    - How we will do text analysis in the coming weeks. Doing things
      to words by using the loop. 
- Loop through breakfast list. 
- Anatomy of the loop: explain syntax.
- Demonstrate this with "hello". For letter in hello, print letter. 
- PRACTICE on your own for 2 minutes. 
- One thing: autmatic variable assignment. Like functions.
- GROUP CHALLENGE: Write some code to print out the square of each of
  these numbers. Remember that the square of a number is that number
  times itself. Remember how to use "f-strings".

  prime_numbers = [2, 3, 5, 7, 11]

No homework for now, only Wednesday next week. Homework will be
collected on blackboard.

*** DONE February 15: Lists & HW
     CLOSED: [2023-02-15 Wed 16:07]
Today's goals: 
- review Loops
- learn about Lists
- start homework (now on blackboard and course website)

Take attendance

Loops
 - How we will work with data. Like functions, core programming
 concepts.
     - Python works with data by /iterating/. The loop.
     - How we will do text analysis in the coming weeks. Doing things
       to words by using the loop.
 - Demonstrate this with "hello". For letter in hello, print letter.

Lists
 - remember lists? create a list of books and print them.
 - ABSTRACTION:
       #+BEGIN_SOURCE python
       books = ['Gender Trouble', 'Cruising Utopia', 'Living a
       Feminist Life']
       # print(books)
       list_length = len(books)
       print(list_length)
       #+END_SOURCE
     - using the len() and print() functions
     - using variables to store data, "argument" "parameter"
 - LIST INDEXING
     - allows you to pick out an item from a list
     - every list has an "index" starts with zero
     - square brackets
     - negative numbers count from the end
 - LIST SLICING
     - using index to take a section of a list
     - `books[0:2]`
     - the colon is a middle ground. everything in between
     - first value is inclusive, second is exclusive.
 - GROUP CHALLENGE
     - Create a new list of books, with at least 5 books in your
       list. Make sure the total number of books in the list is an odd
       number.
     - How do you print out the book in the middle of the list?
     - What about the three books in the middle? Remember that the
       first value in a slice is inclusive, and the final value is
       exclusive.

Go over homework
- on blackboard and course website. 
- will need to download the document. Then open with jupyter or upload
to google colab, depending on what you use. Meant to make it easier.
- if you cannot get the document to work on your computer, just answer
the questions in the correct order. that's fine. 
- due on Tuesday 2pm class.

Tuesday class will be in person, Wednesday will be online. 

*** DONE February 21: Logic
     CLOSED: [2023-02-22 Wed 13:34]
Today's goals: 
- review Lists
- learn about List methods & Logic - conditionals
- last class on python basics. Tomorrow we start text analysis yay!
- installations (checking installs) for NLTK
- Announcement: class on 2/27 (monday) will be cancelled 

REVIEW LISTS
- List indexing 
  - allows you to pick out an item from a list
  - implicit index
  - negative numbers
- List slicing - grab a section of list
  - `books[0:2]`
  - the colon is a middle ground. everything in between
  - first value is inclusive,

TAKE ATTENDANCE

LIST METHODS
   - list is an object, a data type of "list"
   - we can do specific things like: 
   - sort()
   - append() # takes argument
   - pop() 
GROUP CHALLENGE 
   - Research the w3schools tutorial website for one new list method.
 https://www.w3schools.com/python/python_lists_methods.asp 

CONDITIONAL STATEMENTS
- remember booleans? 
- true or false is how a computer makes decisions
- "conditional statement"
  - Conditionals allow programs to change their behavior based on
    whether some statement is true or false.
  - 1 == 1, 1 == 0
  - operators, assignemnt vs equivalence
    - 10 > 5

IF STATEMENTS
#+begin_source python
if 10 > 5: 
   print(f"congrats! the expression {x} > 5 evaluates to
True!")
else:
   print(f"sorry, the expression {x} > 5  is false")
#+end_source

ELIF CLAUSES
#+begin_source python
movie = ""
if movie == "The Witch":
    print("The movie is The Witch")
elif movie == "Braveheart":
    print("The movie is Braveheart")
elif movie == "Dracula":
    print("The movie is Dracula")
else:
    print("I don't know what movie you're talking about! I'm just a little program...")
#+end_source

CHALLENGE: add two more elif statements, and unique print statements
to go with each one. Then, keep changing the "movie" variable so that
you can get different things to print. 

*** DONE February 22 Intro NLTK
     CLOSED: [2023-02-22 Wed 16:27]
Announcements:
- next class (Monday) is cancelled. Will see you Weds
- Schedule: starting Unit 2 on text analysis, 2 weeks
- our first reading is for Wednesday
- adding links to workshop content if you missed class (not a
  substitute, because not exact, but better than nothing)

Today: NLTK, installations

Attendance

NLTK installations
- On google colab, copy/paste the code from each cell. 
- Thereafter, will only need "import nltk" each time you open a
  notebook. 
- If using python on MAC or Windows, follow instructions
  online. Requires command-line. Sign up to meet with me if you are
  getting stuck. 

Intro to NLTK
- Natural Language Toolkit - NLP, fastest growing field. ChatGPT
- Computers to study and use human language. The future. 
- Today, basic building blocks with NLTK. Industry standard. Popular.

#+BEGIN_SOURCE python

# import your library
from nltk.book import *

# variables represent texts
text5

# methods for searching text
text5.concordance('man') # gets us the context
text5.condordance('woman')

text5.similar('man') # calculates based on context
text5.similar('woman')

text4.dispersion_plot(['good', 'evil', 'Adam', 'Eve']) # visualizes
when it appears

#+end_source

GROUPS CHALLENGE
- Get into groups and explore one or more texts from this corpus. 

BACK AS A CLASS:
- What words are you interested in exploring in your selected corpus?

READING HOMEWORK

*** DONE March 1: loading text with NLTK
     CLOSED: [2023-03-06 Mon 11:09]
Announcements:
- will discuss reading next week, on Monday. Extension.

Agenda:
- review NLTK package and default texts
- start working with text we find on internet
- cleaning text, getting ready for analysis

REVIEW: 

#+BEGIN_SOURCE python

## REVIEW:
# import your library
from nltk.book import *

# variables represent texts
text5

# methods for searching text
text5.concordance('man') # gets us the context
text5.condordance('woman')
text5.similar('man') # calculates based on context
text5.similar('woman')
text4.dispersion_plot(['good', 'evil', 'Adam', 'Eve']) # visualizes
when it appears


#+end_source

NEW:

Loading up the libraries

#+BEGIN_SOURCE python

# libraries we need for text analysis and working with URLs
import nltk
from urllib.request import urlopen

# google colab (not for jupyter-notebooks) we need 'punkt' and
# 'stopwords'
nltk.download('punkt')
nltk.download('stopwords')

#+END_SOURCE

Loading up the text

#+BEGIN_SOURCE python

# saves URL and tells python to open it
my_url = ""
saved_url = urlopen(my_url)

# reads the data in the URL
raw = saved_url.read()
# transforms the data into a string format (so we can work with it)
decoded = raw.decode()

# what our text currently looks like
decoded

#+END_SOURCE

tokenizing the text

#+BEGIN_SOURCE python

# turning the long string into a list of "tokens"
# also removes any strange formatting (like the "/n")
tokens = nltk.word_tokenize(decoded)

# let's see the first 10 items, or "tokens"
tokens[:10]

#+END_SOURCE

removing frontmatter & backmatter

#+BEGIN_SOURCE python

# first, we want to remove frontmatter, which we don't need for analysis
# to do that, we need to find *where* the first word of the text appears
# we use the index() function to locate the first word, "He", in this text
tokens.index("first word")

# "He" occurs at location 872, so we slice the text from 872 onward
# this gets rid of the frontmatter 
text = tokens[237:]

text[:10]

#+END_SOURCE

GROUP CHALLENGE:
- recreate this process with a different text from Gutenberg. 

*** DONE March 6: cleaning: punct, caps, stops; intersectionality
     CLOSED: [2023-03-08 Wed 13:30]
Announcements: 
- Homework due Monday, March 8 - will be like a review
- Midterm on *Wednesday, online*. Will open a zoom room and give you
  the assignment. Then you will submit it to Blackboard at 3:15 pm
  (except those of you who have extra time).

Agenda:
- review NLTK methods for loading and tokenizing text
- start cleaning text, getting ready for analysis
- start talking about data feminism book and theoretical concepts for
  the course

REVIEW loading up the text, tokenizing, removing frontmatter: 

#+BEGIN_SOURCE python

import nltk
from urllib.request import urlopen

# FOR JUPYTER
from nltk.corpus import stopwords

# FOR COLAB
nltk.download('punkt')
nltk.download('stopwords')

# LOAD AND TOKENIZE TEXT

# saves URL and tells python to open it
my_url = ""
saved_url = urlopen(my_url)

# reads the data in the URL
raw = saved_url.read()
# transforms the data into a string format (so we can work with it)
decoded = raw.decode()

# what our text currently looks like
decoded

# turning the long string into a list of "tokens"
# also removes any strange formatting (like the "/n")
tokens = nltk.word_tokenize(decoded)

# let's see the first 10 items, or "tokens"
tokens[:10]

# FRONT AND BACKMATTER

# "He" occurs at location 872, so we slice the text from 872 onward
# this gets rid of the frontmatter 
text = tokens[237:]

#+END_SOURCE

REMOVE PUNCTUATION & CAPITAL WORDS

#+BEGIN_SOURCE python

# to avoid punctuation and capitalization skewing our word counts, 
# we remove punctuation and make everything lowercase
# for that, we use a loop, and an if statement. 

no_punct = []
for word in text:
  if word.isalpha():
    no_punct.append(word.lower())

no_punct[:20]

#first create an empty list
no_punct = []
# then create a loop that goes through every word in the text.
for word in text:
  # checks if that word is an alphabetic word
  if word.isalpha():
    # if it is alphabetic, lowercase the word, and append it to our new list.
    no_punct.append(word.lower())

#+END_SOURCE

REMOVE STOPWORDS: 

#+begin_source python
stops = stopwords.words('english')
stops

o_unstopped = []
for t in o_text:
    if t not in stops:
        o_unstopped.append(t)
#+end_source

DATA:
- what was this chapter about? Darden? 
- we will be looking at data analysis methods in this class
- data analysis methods have been historically used by those in power
  to maintain power or exert power. 
- FREEWRITE: in what ways is data about power? who controls data? what
  do the people who control data do with it?
- SHARE AND MAKE LIST

INTERSECTIONALITY:
- so how do we fight this power? We have feminism, have been doing
  this for centuries. But feminism isn't always right, either. 
- first, second, third waves. Critiques of each other. 
- intersectional feminism is the most effective, according to the
  authors. 
- Krenshaw, hooks. 
- in what ways does data analysis perpetuate discrimination? This is
  the question this course will try to answer from now on.

*** DONE March 8: cleaning continued
     CLOSED: [2023-03-08 Wed 16:30]
Agenda:
- continue lesson about cleaning
- continue talking about data
- start homework in class
- midterm wednesday, Monday is review. 

Monday, started "cleaning" with removing punctuation and lowercasing
letters 
- why would we want to clean the text?
- regularize the text so word counts are accurate.
- what be aware of what gets taken out. The details. 

REVIEW: REMOVE PUNCTUATION & CAPITAL WORDS

#+BEGIN_SOURCE python

# to avoid punctuation and capitalization skewing our word counts, 
# we remove punctuation and make everything lowercase
# for that, we use a loop, and an if statement. 

no_punct = []
for word in text:
  if word.isalpha():
    no_punct.append(word.lower())

no_punct[:20]

#first create an empty list
no_punct = []
# then create a loop that goes through every word in the text.
for word in text:
  # checks if that word is an alphabetic word
  if word.isalpha():
    # if it is alphabetic, lowercase the word, and append it to our new list.
    no_punct.append(word.lower())

#+END_SOURCE

REMOVE STOPWORDS

Stopwords: words that occur so frequently they will skew our counts. 

First, I will show you why:

#+BEGIN_SOURCE python

from nltk import FreqDist
FreqDist(text).most_common(20)

#+END_SOURCE

So how do we remove stopwords? 

#+begin_source python
stops = stopwords.words('english')
# on google colab: nltk.download('stopwords')

o_unstopped = []
for t in o_text:
    if t not in stops:
        o_unstopped.append(t)
#+end_source

LEMMATIZING

Reducing word forms to the root

#+begin_source python

# Colab: importing the code we need to lemmatize
from nltk.stem.wordnet import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')
lemmatizer = nltk.stem.WordNetLemmatizer()

# Jupyter
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

wordnet_lemmatizer.lemmatize("children")

wordnet_lemmatizer.lemmatize("better")

wordnet_lemmatizer.lemmatize("better", pos='a')

# let's do it to the whole text

text_lemmatized = []
for word in text_no_stops:
  word_l = lemmatizer.lemmatize(word)
  text_lemmatized.append(word_l)


#+end_source

NLTK.TEXT object

This is the final step, turning it into a text object. 

#+BEGIN_SOURCE python
orl = nltk.Text(orl_lemmatized)
type(orl)
#+end_source 

Now, explore these methods from before like similar, etc.

Do some analysis in a group:
- Based on the results, what can you infer?
- Based on the results, what questions do you have for more analysis? 

*** DONE March 13: Midterm review
     CLOSED: [2023-03-15 Wed 14:42]

*** DONE March 15: Midterm
     CLOSED: [2023-03-15 Wed 14:42]
*** DONE March 20: NLTK continued
    CLOSED: [2023-03-27 Mon 11:42]
Goals:
- exploring the Gutenberg corpus to ask questions about data
- setting us up for data analysis, final projects. 

Lecture on Text Analysis - 20 minutes: 
- old methodology, hundreds of years. Concordances in Shakespeare.
- now super important/relavant for handling big data. All of our apps
  use "NLP", and we are building programs toward "NLU" - natural
  language understanding.
  - NLTK has popular NLP algorithms built-in. NER and POS. 
  - these are for "information extraction." 
  - how do these work? They have dictionaries with the info. NLTK
  looks them up.
- But this is not the way of the future. The way of the future is
  AI for working with text like NLP. One subset is "generative AI",
  which we have with Chat GPT. 
- A computer understands words. How does it understand? Any guesses?
- Based on a foundational task, which you have been working on
  learning, even if you don't realize it yet. 
  - how to make inferences from patterns. "Inferences" "Patterns"
  - word frequencies is the key. Frequencies of frequencies. 
  - A computer can can learn existing patterns by counting. It can
    then make predictions. This is all machine learning, AI.
    - this is how something like ChatGPT works. A word embedding is a
      probability that some word is associated with another word. To
      get this, we need a context window.
  - You see how important it is to get good data here? Why is it
      important? 
- In this class, we're going to look at exactly what kinds of results
  we get from frequencies. Back into the NLTK methods, to look closely
  at them. 

Personals corpus - 20 minutes
+ similar, concordance, common_contexts, distribution_plot, 
+ common_contexts([])
+ collocations()

*** DONE March 22: NLTKs research methods
    CLOSED: [2023-03-27 Mon 11:42]
Review nltk cleaning and methods
Some new methods in NLTK
Continue looking for patterns

Review NLTK cleaning and methods - 45 min
- cleaning the text
- similar, concordance, common_contexts, distribution_plot, 
- common_contexts([])
- collocations()

Groups - 10 min
- play around with these methods. 
- what *patterns* are you picking up about your data? Are there
  certain things you're noticing about the data? Perhaps thinking
  about the ways men and women are described?
- what kinds of further questions could you ask about this data? 

Share - 10 min

*** TODO March 27
Text analysis of gutenberg books
Developing research questions from these patterns
---
Discuss reading:
- What is ChatGPT? What is a language model?
 - a program that has "read" or been "trained" by lots of text so it
   knows text patterns.
 - one example is statistical models. A language model learns the
   probability of word occurrence based on examples of text. similar()
   is like a statistical model.
 - example is next word prediction like ChatGPT, neural model. Does
   the same thing, but at a much larger scale. Uses "word embeddings"
   -- each word contains a vector of values.

- Bias 
 - What is bias?
 - What was the bias in ChaptGPT?
 - Why is ChatGPT biased?
  - trained on internet, "Common Crawl"
  - it's biased because we are biased.
 - What should we do about this problem? Freewrite 5 minutes. 

Analyzing gender with NLTK
- select a text
- load and clean it
- use NLTK methods to explore the text
 - similar, concordance, common_contexts, distribution_plot, 
 - common_contexts([])
 - collocations()

In groups - 15 min:
- look at text by men versus text by women. Choose one text from each
  category for your group. 
- look at male and female characters within these texts. You may have
  to do some research. 
- explore the different associations between male and female
  characters in the two texts. What words are associated with each
  gender? 
- What are the differences you see between men and women characters? 
- What are the differences you see between the male and female
  authors?

Share - 10 min

*** 27, 29: 
*** April 3: Catch-up day
*** SPRING BREAK -- April 5 - 12
*** April 17, 19, 24: Text modelling with Transformers 
*** April 26, May 1, 3: Workshop final projects





